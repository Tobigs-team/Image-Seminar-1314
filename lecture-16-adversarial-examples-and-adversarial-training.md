---
description: 14기 서아라
---

# \[Lecture 16\] Adversarial Examples and Adversarial Training

안녕하세요, 투빅스 14기 서아라입니다.

오늘 리뷰할 강의의  Adversarial Examples and Adversarial Training입니다.

아직 공부하는 단계이기도하고, 부족한 부분이 많아서 다소 맞지 않는 부분도 있을까 염려됩니다.

그래도 열심히 정리해보도록 하겠습니다. 도움이 되시길 바랍니다.



오늘 강의의 목차입니다.

우선 저희는 adversarial examples가 무엇인지에 대하여 배울 예정입니다.

그런 다음 그것이 왜 일어나는 지에 대하여 알아볼 것입니다.

그리고 그것이machine learning systems에서 어떤 위협을 주는지, 방어 대책은 있는지, 마지막으로 adversary가 없는 상황에서 adversarial examples들이 어떻게 machine learning의 성능을 향상시키는 지 등에 대하여 알아볼 것입니다.

Adversarial Examples를 설명할 때 가장 많이 쓰이는 이미지입니다.

사람은 너무 당연하게도 양 쪽의 사진이 모두 팬더라는 것을 압니다.

그러나 컴퓨터 같은 경우는 왼쪽의 사진에 가운데의 노이즈가 0.07의 비율로 포함된 오른쪽의 사진을 '긴코 원숭이'로 인식한다고 합니다.

놀랍게도, 팬더라고 인식했을 때\(57.7%\) 보다 더 높은 확신\(99.3%\)을 가지고 말입니다.

이와 같이 Adversarial attack이란 성능이 좋은 DNN을 이용한  Classifier들에 적대적 교란\(adversarial pertubation\)을 적용할 경우, 분류 알고리즘들이 쉽게 잘못된 분류를 할 수 있도록 하는 것을 의미합니다.



--4p 나중에 정리--

이게 왜 일어나는 상황인가에 대하여 강사님의 동료분께서 실험\(?\)을 하는 과정에서 얻어낸 이미지들입니다.

한 박스에 붙어 있는 이미지가 저희가 보기에는 모두 같은 이미지처럼 보입니다.

하지만 해당 이미지들을 모두 numpy형태의 숫자로 바꾼 후, 차이를 보면 non-zero값이 나온다고 합니다.\(즉, 다른 이미지라는 의미입니다.\)

우리가 보기에 그리 큰 차이가 없어 보이는 이미지들인데,

신기하게도 한 박스 안에서 오른쪽 아래로 갈수록, 해당 이미지를 '비행기'에 가깝게 인식한다고 합니다.

우리는 보통 딥러닝이 비행기를 인식하는 방법은 '푸른 하늘과, 날개가 있을 때' 비행기라고 인식하는 것이라고 생각합니다.

물론 해당 내용도 맞겠지만, 실제로는 '그렇지만은 않다'라는 것입니다.

다른 이유로 해당 이미지를 '비행기'로 인식하기도 한다는 것입니다.

그리고, 이러한 이미지의 변형을 일으키는 것이 그리 어려운 것이 아니라고 합니다.

강사님의 동료 분도 단순히 네트웨크에 '비행기의 이미지를 만들어 달라'고 지시하기만 했고, 쉽게 비행기라고 착각할 수 있도록 하는 이미지를 만들어 주었다고 합니다.

생각보다 이러한 과정이 어렵지 않고 쉽다는 것은 쉽게 악용이 될 수 있다는 것과 같은 의미이기 때문에 문제입니다.



옆에 보이는 9로 이루어져 있는 이미지는, 딥러닝이 아닌 얕은 머신러닝 모델에 적용되어졌다고 합니다.

노란색 네모박스로 쳐진 숫자들은 왼쪽에서 오른쪽으로, 위에서 아래로 갈수록 차례로 0,1,2,...,9의 숫자를 나타내었다고 합니다.

마찬가지로 이미지의 숫자 9의 모양을 변형하지도 않았는데 다른 숫자로 인식하는 상황이 일어났고, 이는 linear model뿐만 아니라 다른 SVM이나 decision tree, nearest neighbors classifiers에도 해당하였다고 합니다.



그동안 우리는 이러한 상황을 'over fitting'으로 설명해왔습니다.

딥러닝 모델이 너무 complicated하여 구불구불한 나머지, training set에 대하여는 잘 분류를 하였으나, test set에 대해서는 잘 분류를 하지 못하는 것이라는 의미입니다.

하지만, 만약 진짜로 overfitting의 문제라면, 각각의 adversarial examples에 대하여 다른 모델들도 random한 결과를 내야하는데, 그게 아니라 모두 같은 데이터에 대하여 같은 실수를 하고 있었습니다.

따라서 이것에 대하여 overfitting이 아닌 underfitting의 문제로 설명하기 시작하였습니다.

그리고 모델이 underfitting이 된 이유는 모델이 부분적으로 'linear'하기 때문으로 설명하였습니다.

여러 레이어를 쌓기 때문에 완전히 linear한 것은 아니지만, ReLU, Maxout, Carefully tuned sigmoid, LSTM등을 사용하기 때문에 piecewise linear하다는 것입니다,



