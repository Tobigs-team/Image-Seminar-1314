---
description: 13기 강미경
---

# \[Paper Review 2\] You Only Look Once Unified, Real-Time Object Detection

![](.gitbook/assets/image%20%2849%29.png)



여러가지 물체들의 위치와 그 물체가 무엇인지 찾아내는 것! 

 Multiobject : Localization + Classification



![](.gitbook/assets/image%20%28122%29.png)

응급상황 혹은 범죄 등, 이상현상을 실시간으로 감지하는 데 사용될 수도 있습니다.

경기 동안 이뤄지는 선수들의 동선 등을 분석할 수도 있겠고요,

무인 점포 시스템에도 이용할 수 있습니다. 실제로 아마존에서도 Amazon Go라는 이름으로 무인 점포를 출시하였다고 합니다. 아직은 완전 무인은 아니라고는 하지만 획기적인 시도였다고 생각합니다. 여기서는 사진에 나와있는 것처럼 진열장 안의 물체들의 위치 뿐만 아니라 사진은 없지만 고객이 들어와서 어떤 물건을 가지고 나가는지 까지 object detection으로 가능하다고 합니다. 그래서 가지고 나가는 물건 자동결제 되는 시스템으로요!





![](.gitbook/assets/image%20%2893%29.png)

a. 결론적으로 이 지난번 홍정님의 강의와, 오늘 제가 리뷰해드릴 모델인 YOLO가 수행하는 object detection이라는 태스크는 일상 생활에서 굉장히 많이 사용되기 때문에 잘 알아두어야 할 테스크라고 할 수 있습니다! b. 또, 앞선 예제들의 공통점이 있는데, 바로 ‘실시간\(real-time\)’이라는 것입니다. 실시간으로 들어오는 영상을 처리하는데 가장 중요한 것은 바로 1초에 몇 개의 프레임을 처리할 수 있느냐에 대한 것, 바로 speed 입니다. 오늘 배울 YOLO가 바로 이 스피드에 초점을 맞춘 모델이기도 하죠!



![](.gitbook/assets/image%20%2827%29.png)

Object Detection 모델은 크게 두 가지로 나눌 수 있다고 배웠었죠. 바로 이 그림으로요. 크게, 1-stage detector과 2-stage detector로 나눌 수 있고, 우리가 지지난 시간에 배운 R-CNN 패밀리가 바로 2-stage 모델에 속하고, 오늘 배울 YOLO는 1stage 모델입니다. 각 형태의 모델에 어떤 큰 차이가 있는지 볼게요.

![](.gitbook/assets/image%20%2823%29.png)

[https://ganghee-lee.tistory.com/34](https://ganghee-lee.tistory.com/34)  
먼저 2stage Detector부터 보겠습니다. 우리 지난 시간에 배웠던 R-CNN 패밀리들을 기억해보세요. 이들은 Region Proposal과 Classification을 순차적으로 하는 모델로, Localization을 통해 물체의 위치를 찾아내는 부분과, 그 물체가 어떤 클래스에 해당되는지 맞추는 부분이 따로 있으며, Region을 찾고 그 찾은 Region에 대해 클래스를 분류하는, 즉 순차적으로 처리가 된다는 특징이 있었습니다.  
이 Region Proposal 부분은 대표적으로 Selective Search가 있었죠! 혹시 기억하실까요? 이게 막 CPU로 연산되기 때문에 매우 느려서 나중에 가서는 이를 Region Proposal Network 라고 해서 네트워크로 만들어서 학습시키는 형태로 바꾸기도 했었는데요, 여하튼 Region Proposal에는 이러한 방식들이 있었습니다.  
이 2 stage 모델은 정확도는 비교적 잘나오는데, 두 부분으로 나눠져 있기도 하고, 그 bounding box를 찾아내는 region proposal을 하는 것 자체가 느리기 때문에 전체적으로 스피드가 많이 떨어진다는 단점이 있었습니다.  


![](.gitbook/assets/image%20%2841%29.png)

1-stage 모델에서는 Region Proposal과 Classification이 동시에 이루어집니다. 구조도 되게 간단해보이죠! 이 1-stage 모델은 2-stage 기반 방식의 문제점이었던, region proposal에 시간이 오래 걸린다는 단점을 극복하고자 제안된 모델로, 굉장히 빨라서 실시간 영상도 곧잘 처리합니다. 단, 스피드에 투자를 하다보니 2-stage 모델보다 정확도는 조금 떨어진다는 특징이 있습니다. YOLO는 단 하나의 Convolution Nueral Network를 모델로 쓰는데, 이것의 출력인 Output Feature map에는 Region proposal로 예측된 바운딩 박스에 대한 정보도 가지고 있고, 그 박스 안에 물체가 있는지, 있다면 어떤 물체인지에 대한 정보도 다 가지고 있다고 합니다. 이제! 그 욜로가 어떤 형태인지 알아볼 것인데, 바로 그 전에 우리 지난 시간에 배웠던 2-stage detector들이 워낙 중요한 내용이기도 하고, 이것들에 대한 개념이 잘 잡혀있어야 욜로도 잘 이해할 수 있기 때문에 간략하게 짚어보고 넘어가겠습니다.



![](.gitbook/assets/image%20%2895%29.png)

[https://jaehyeongan.github.io/2019/10/10/R-CNN/](https://jaehyeongan.github.io/2019/10/10/R-CNN/)  
Region Proposal 하는 부분과, CNN이 결합되었다고 해서 붙여진 이름.   
한 마디로 전체 구조가, region proposal을 하는 부분, 그 결과들로 각각 cnn을 돌려 feature를 추출하는 부분, 추출된 피쳐를 바탕으로 classification과 Box Regression을 하는 부분으로 볼 수 있습니다.   
Region Proposal : Selective Search 이용, 이는 객체와 주변간의 색감, 질의 차이 등을 파악해서 유사도를 기반으로 박스를 만들어나가는 알고리즘으로, 매우 좋지만  CPU 연산으로만 가능하기 때문에 느리다는 단점이 있었죠. 그리고 이 부분은, 그저 알고리즘으로 학습되는 네트워크가 아니었습니다.

![](.gitbook/assets/image%20%2818%29.png)





![](.gitbook/assets/image%20%28107%29.png)



![](.gitbook/assets/image%20%289%29.png)



![](.gitbook/assets/image%20%2839%29.png)







![](.gitbook/assets/image%20%2832%29.png)





![](.gitbook/assets/image%20%2861%29.png)



![](.gitbook/assets/image%20%28101%29.png)



![](.gitbook/assets/image%20%286%29.png)



![](.gitbook/assets/image%20%2824%29.png)



![](.gitbook/assets/image%20%2891%29.png)







![](.gitbook/assets/image%20%2812%29.png)



![](.gitbook/assets/image%20%2872%29.png)



![](.gitbook/assets/image%20%2870%29.png)

![](.gitbook/assets/image%20%2817%29.png)



![](.gitbook/assets/image%20%2863%29.png)



![](.gitbook/assets/image%20%2853%29.png)



![](.gitbook/assets/image%20%2813%29.png)

![](.gitbook/assets/image%20%2880%29.png)

![](.gitbook/assets/image%20%2835%29.png)

![](.gitbook/assets/image%20%2865%29.png)

![](.gitbook/assets/image%20%283%29.png)

![](.gitbook/assets/image%20%2833%29.png)

![](.gitbook/assets/image%20%2882%29.png)

![](.gitbook/assets/image%20%28119%29.png)

![](.gitbook/assets/image%20%28120%29.png)

![](.gitbook/assets/image%20%2810%29.png)

![](.gitbook/assets/image%20%2887%29.png)

![](.gitbook/assets/image%20%28111%29.png)

![](.gitbook/assets/image%20%28114%29.png)

![](.gitbook/assets/image%20%2829%29.png)

![](.gitbook/assets/image%20%2859%29.png)

![](.gitbook/assets/image%20%2877%29.png)

![](.gitbook/assets/image%20%28127%29.png)

![](.gitbook/assets/image%20%2848%29.png)

![](.gitbook/assets/image%20%28121%29.png)

![](.gitbook/assets/image%20%2894%29.png)

![](.gitbook/assets/image%20%28104%29.png)

![](.gitbook/assets/image%20%2898%29.png)

![](.gitbook/assets/image%20%2889%29.png)

![](.gitbook/assets/image%20%2899%29.png)

![](.gitbook/assets/image%20%2858%29.png)

![](.gitbook/assets/image%20%2840%29.png)

![](.gitbook/assets/image%20%2822%29.png)

![](.gitbook/assets/image%20%2873%29.png)

![](.gitbook/assets/image%20%2884%29.png)

![](.gitbook/assets/image%20%2869%29.png)

![](.gitbook/assets/image%20%2814%29.png)

![](.gitbook/assets/image%20%2828%29.png)



![](.gitbook/assets/image%20%2897%29.png)

![](.gitbook/assets/image%20%2852%29.png)

![](.gitbook/assets/image%20%2885%29.png)

![](.gitbook/assets/image%20%2879%29.png)



![](.gitbook/assets/image%20%28116%29.png)

![](.gitbook/assets/image%20%28109%29.png)

![](.gitbook/assets/image%20%2850%29.png)

![](.gitbook/assets/image%20%2886%29.png)

![](.gitbook/assets/image%20%28100%29.png)



![](.gitbook/assets/image%20%28115%29.png)

![](.gitbook/assets/image%20%2820%29.png)







